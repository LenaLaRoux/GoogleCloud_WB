import datetime as dt
from os import environ
from time import sleep
from re import finditer
from  requests import Session
from json import loads
from json import dumps

from google.auth import default
from google.cloud import bigquery
from google.cloud.exceptions import NotFound

SCHEMA =[
        bigquery.SchemaField("token", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("odid", "INTEGER", mode="REQUIRED"),
        bigquery.SchemaField("gNumber", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("date", "TIMESTAMP", mode="NULLABLE"),
        bigquery.SchemaField("lastChangeDate", "TIMESTAMP", mode="NULLABLE"),
        bigquery.SchemaField("supplierArticle", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("techSize", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("barcode", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("quantity", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("totalPrice", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("discountPercent", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("warehouseName", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("incomeId", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("nmId", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("subject", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("category", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("brand", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("oblast", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("number", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("isCancel", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("cancel_dt", "TIMESTAMP", mode="NULLABLE")
    ]

def create_or_uppend_orders (data, context):

  default_start_date = dt.datetime(2020, 6, 1)
  
  client, project_id = init_client()

#   project_id = "wbdataloader"   
#   client = init_client_(project_id)

  token, dataset_name, table_name, url, max_rows = init_attrs(data) 
  dataset_temp_name = dataset_name+"_TEMP" 
  
  check_dataset_existance(client, project_id, dataset_name)
  #Temporary tables will be deleted in 3 hours
  check_dataset_existance(client, project_id, dataset_temp_name, int( dt.timedelta(hours=3)/ dt.timedelta(milliseconds=1)))

  #Create or update main table
  table_id = '{0}.{1}.{2}'.format(project_id, dataset_name, table_name)
  table_temp_id = '{0}.{1}.{2}'.format(project_id, dataset_temp_name,  "_".join([table_name, token])) 

  while True:
            #rows = check_table_existance(client, table_id, token)
            row = get_last_added_row(client, table_id, token)
            #get last cgange date to start upload data with
            last_change_date = row['lastChangeDate'] if row else default_start_date 
            params = prepare_params(token, last_change_date,  0)
          
            #upload data from api web service
            data = make_request(url, params)
            
            if  data: 
                #Process recieved result by smaller parts to exclude Out Of MEMory
                short_data = []
               
                for match in finditer(r'\{.+?\}', data ):
                  
                    obj_str = match.group()
                    obj = loads(obj_str)
                    #Add token for filter in a table
                    obj =  {**obj, 'token': token} 

                    short_data.append(obj)

                    if (max_rows and max_rows > 0 and len(short_data) ==  max_rows):
                        process_data(short_data, client, table_id, table_temp_id, token, row) 
                        short_data=[]   

                if len(short_data) > 0:
                    process_data(short_data, client, table_id, table_temp_id, token, row) 
            
            else:
                break
            
            sleep(10)

  print("\nFunction finished successfuly")     

def process_data (data, client, table_id, table_temp_id, token, exists):
      # if table already exist then create temporary table to delete dublicates from table
          if exists:
                job_config_temp = bigquery.LoadJobConfig(
                              schema = SCHEMA,                
                              autodetect = False,
                              create_disposition = "CREATE_IF_NEEDED",
                              write_disposition = 'WRITE_TRUNCATE'
                              )

                #table_temp_id = '{0}.{1}.{2}'.format(project_id, dataset_temp_name,  "_".join([table_name, token])) 
                load_job(client, table_temp_id, job_config_temp, data)
                  
                #delete rows from main table that contains Temp table to avoid dublicates by (odid, saleID)
                delete_from_main_temptable(client, table_id, table_temp_id, token)
                #client.delete_table(table_temp_id, not_found_ok=True)  # Make an API request.
                #print("Deleted temporary table '{}'.".format(table_temp_id))

          #upload data from json
          job_config = bigquery.LoadJobConfig(
                        schema = SCHEMA,                
                        autodetect = False,
                        create_disposition = "CREATE_IF_NEEDED",
                        write_disposition = 'WRITE_APPEND',
                        clustering_fields = ['token'],
                        time_partitioning = bigquery.TimePartitioning(type_="MONTH", field="lastchangedate", expiration_ms=None)
                        )
          load_job(client, table_id, job_config, data)



def check_dataset_existance(client, project_name, dataset_name, expiration_duration=None):
    dataset_id = '{0}.{1}'.format(project_name, dataset_name)
    dataset = None
    try:
        dataset =  client.get_dataset(dataset_id)  # Make an API request.
        print(f"Dataset {dataset_name} already exists. Use append mode")
        
    except NotFound:
        print(f"Dataset {dataset_name} doesn't exist. Will create the dataset ", dataset_id)
        dataset = bigquery.Dataset(dataset_id)
        dataset.location = "europe-west3"
        dataset = client.create_dataset(dataset)  # Make an API request.

    if dataset is None:
      raise NotFound ('Dataset '+dataset_id+' was not created')

    update_default_expiraton(client, dataset, expiration_duration)
    return dataset



def update_default_expiraton (client, dataset, expiration_duration=None):
  
  if dataset.default_table_expiration_ms != expiration_duration or dataset.default_partition_expiration_ms is not None:
    print("Update default table expiration: "+str(expiration_duration))
    print("Update default partition expiration: " + str(expiration_duration))

    # delete default table expiration and partition expiration
    dataset.default_table_expiration_ms = expiration_duration  # In milliseconds.
    dataset.default_partition_expiration_ms = None

    # update dataset 
    dataset = client.update_dataset(  dataset, ["default_table_expiration_ms", 'default_partition_expiration_ms'] )  # Make an API request.


def check_table_existance(client, table_id, token):
    # try:
    #     table = client.get_table(table_id)  # Make an API request.
    #     print(f"Table {table_id} already exists.")
        
    #     return table
    # except NotFound:
    #     print(f"Table {table_id} doesn't exist. Will create the table ")
    #     return None
    print("/nSearch for Table: "+ table_id)
    query = """
        SELECT * FROM `{}` 
        where 
          token='{}'
        ORDER BY lastChangeDate desc
        --LIMIT 1
        """.format(table_id, token)
    
    query_job = client.query(query)  # Make an API request.
    results = query_job.result()  # Wait for query to complete.
    print("Got {} rows.".format(results.total_rows))

    return results


def get_last_added_row(client, table_id, token):
    print("\nSearch for last added row")
    try:
        query = """
            SELECT * FROM `{}` 
            where 
              token='{}'
            ORDER BY lastChangeDate desc
            LIMIT 1
            """.format(table_id, token)
        
        query_job = client.query(query)  # Make an API request.
        
        for row in query_job:
          return row

    except NotFound:
       print(f"Table {table_id} doesn't exist. Will create the table ")
    
    return None


def prepare_params( token, datefrom, flag=0):  
    params={}
    params['key'] = token
    params['datefrom'] = datefrom.isoformat(timespec='milliseconds')
    params['flag'] = flag 

    return params


def make_request (url, params):
    print ("\nOpen request session")
    rq_client = Session()

    print("-- Parameters: "+"".join([("\n---- key=" + key + ", value=" + str(value)) for key, value in params.items()]))
    print("---- Used URL: " + url)

    response = rq_client.get(url=url,
                            params=params,
                            headers={'Content-Type': 'application/json'})

    if not response.ok :
              raise Exception(" ".join(['---- Request error,  code:', str(response.status_code), response.reason]))
    print ("------ Response is recieved.\nClose request session") 
    rq_client.close() 

    print ("\nStart parse response to JSON data")

    if  response.text == "[]":
        print ("---- No data to append")
        return None
    
    return  response.text


def load_job(client, table_id, job_config, data):
    print(" ".join([str(len(data)) , 'rows are prepared to upload to table', table_id]))
    
    load_job = client.load_table_from_json(data, table_id, job_config=job_config)  # Make an API request.    
  
    print(" ".join(["---------- Created JOB ID =", str(load_job.job_id), "at", str( load_job.created)]))
    try:
        load_job.result()  # Waits for the job to complete.
        print(" ".join(['------------ JOB STATUS:', load_job.state,
            "\n------------ Rows loaded:", str( load_job.output_rows),
            "\n------------ Finished at", str(load_job.ended) ])
        )
      
    except Exception  as e: 
        print(" ".join(['------------Errors: ', dumps(load_job.errors)]))
        raise e

    return load_job


def delete_from_main_temptable (client, main_table_id, temp_table_id, token):
    print("Start delete duplicates")

    job_config = bigquery.QueryJobConfig(
        # Run at batch priority, which won't count toward concurrent rate limit.
        priority=bigquery.QueryPriority.BATCH
    )

    query_dml = """
                delete from `{}` as main
                where exists (
                    select * from `{}`
                    where 
                      odid = main.odid 
                      --and saleid = main.saleID 
                      and main.token = '{}'
                      and main.lastChangeDate between 
                            (select min(date) from `{}`)
                             and
                            (select max(lastChangeDate) from `{}`)
                )
                """.format(main_table_id, temp_table_id, token, temp_table_id, temp_table_id)
    
    query_job = client.query(query_dml, job_config)  # Make an API request.
    print(" ".join(["---------- Created JOB ID =", str(query_job.job_id), "at", str( query_job.created)]))


    try:
        query_job.result()  # Waits for the job to complete.
        print(" ".join(['------------ JOB STATUS:', query_job.state,
            "\n------------ Rows deleted:", str( query_job.num_dml_affected_rows),
            "\n------------ Finished at", str(query_job.ended) ])
        )
      
    except Exception  as e: 
        print(" ".join(['------------Errors: ', *query_job.errors]))
        raise e

def init_client():

    storage_credentials, project = default()
    client = bigquery.Client(project=project, credentials=storage_credentials)

    return client, project


def init_attrs (data):

    if data is None or data['attributes'] is None:
        raise NotFound("Attributes are not set")

    attrs = data['attributes']

    if attrs['DATASET'] is None:
        raise NotFound("DATASET is not set")

    dataset_id = attrs['DATASET']

    if attrs['TOKEN'] is None:
        raise NotFound("TOKEN is not set")
    
    token = attrs['TOKEN']

    table_id = environ.get('TABLE', 'Specified environment variable is not set.')
    url = environ.get('URL', 'Specified environment variable is not set.')
    max_rows = environ.get('MAXROWS', 'Specified environment variable is not set.')

    return [token, dataset_id, table_id, url, int(max_rows)]

#depracate
def delete_update_dates(client, table_id):
    dml_statement = """delete from `{}`as Sales
            where  cast(Sales.date as date)  between DATE_SUB(CURRENT_DATE('Europe/Moscow'), interval 1 DAY) 
            and   CURRENT_DATE('Europe/Moscow')""".format(table_id)

    query_job = client.query(dml_statement)  # API request
    query_job.result()  # Waits for statement to finish
    
    print("Dublicated rows are delited")
